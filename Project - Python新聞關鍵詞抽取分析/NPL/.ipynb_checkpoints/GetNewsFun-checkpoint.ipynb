{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import re\n",
    "import glob, os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apple News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appleNews(SearchString, StartDate, EndDate, rep):\n",
    "    \n",
    "    SearchPage = 0\n",
    "    exit_flag = False\n",
    "    \n",
    "    while (True):\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        SearchPage = SearchPage+1\n",
    "\n",
    "        url = (\n",
    "               \"https://tw.appledaily.com/search/ajaxresult?querystrS={}\".format(SearchString) + \n",
    "               \"&sort=time\" +\n",
    "               \"&searchType=all\" +\n",
    "               \"&dateStart={}%2F{}%2F{}\".format(StartDate.year, StartDate.month, StartDate.day) +\n",
    "               \"&dateEnd={}%2F{}%2F{}\".format(EndDate.year, EndDate.month, EndDate.day) + \n",
    "               \"&page={}\".format(SearchPage)\n",
    "              )\n",
    "\n",
    "        webdata = requests.get(url).text\n",
    "\n",
    "        reqsjson = json.loads(webdata)\n",
    "        \n",
    "        for content in reqsjson:\n",
    "            article_title = content['title']\n",
    "            article_publish_date = content['pubDate']\n",
    "            article_data = requests.get(content['sharing']['url']).text\n",
    "\n",
    "            article_data_suop = BeautifulSoup(article_data, 'html.parser')\n",
    "\n",
    "            article_text = article_data_suop.find('div',class_='ndArticle_margin').p.get_text()\n",
    "\n",
    "            # use these three lines to do the replacement for non-chinese words\n",
    "            rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "            pattern = re.compile(\"|\".join(rep.keys()))\n",
    "            article_text = pattern.sub(lambda m: rep[re.escape(m.group(0))], article_text)\n",
    "\n",
    "            article_text = re.sub(r'[^\\u4e00-\\u9fa5]', \" \",article_text) \n",
    "            cn_pattern = re.compile(r'[\\u4e00-\\u9fa5,\\u0020]')\n",
    "            article_cn_text = \"\".join(cn_pattern.findall(article_text))\n",
    "            \n",
    "            num_article_publish_date = datetime.datetime.strptime(article_publish_date,'%Y%m%d')\n",
    "            \n",
    "            if StartDate <= num_article_publish_date <= EndDate:\n",
    "                saveData(SearchString, article_title, article_publish_date, article_cn_text)\n",
    "\n",
    "\n",
    "        if num_article_publish_date < StartDate or webdata == \"[]\":\n",
    "            break      \n",
    "\n",
    "    print ('done all')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNA News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnaNews(SearchPage, StartDate, EndDate, rep):\n",
    "    \n",
    "    SearchPage = 0\n",
    "\n",
    "    while (True):\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        SearchPage = SearchPage+1\n",
    "\n",
    "        url = (\n",
    "               \"https://www.cna.com.tw/cna2018api/api/simplelist/searchkeyword/{}\".format(SearchString) + \n",
    "               \"/pageidx/{}/\".format(SearchPage)\n",
    "              )\n",
    "\n",
    "        webdata = requests.get(url).text\n",
    "\n",
    "        reqsjson = json.loads(webdata)['result']['SimpleItems']\n",
    "\n",
    "        for content in reqsjson:\n",
    "\n",
    "            article_title = content['HeadLine']\n",
    "\n",
    "            article_publish_date = content['CreateTime']\n",
    "\n",
    "            article_data = requests.get(\"https://www.cna.com.tw\"+ content['PageUrl']).text\n",
    "\n",
    "            article_data_suop = BeautifulSoup(article_data, 'html.parser')\n",
    "\n",
    "            article_paragraph_list = article_data_suop.find('div',class_='paragraph').find_all('p')\n",
    "            article_text_list = []\n",
    "\n",
    "            for article_paragraph in article_paragraph_list:\n",
    "                article_text_list.append(article_paragraph.get_text())\n",
    "\n",
    "            article_text = ''.join(article_text_list)\n",
    "\n",
    "            # use these three lines to do the replacement\n",
    "            rep = dict((re.escape(k), v) for k, v in rep.items())\n",
    "            pattern = re.compile(\"|\".join(rep.keys()))\n",
    "            article_text = pattern.sub(lambda m: rep[re.escape(m.group(0))], article_text)\n",
    "\n",
    "            article_text = re.sub(r'[^\\u4e00-\\u9fa5]', \" \",article_text) \n",
    "            cn_pattern = re.compile(r'[\\u4e00-\\u9fa5,\\u0020]')\n",
    "            article_cn_text = \"\".join(cn_pattern.findall(article_text))\n",
    "\n",
    "\n",
    "            num_article_publish_date = datetime.datetime.strptime(article_publish_date,'%Y/%m/%d %H:%M')\n",
    "\n",
    "            if StartDate <= num_article_publish_date <= EndDate:\n",
    "                saveData(SearchString, article_title, article_publish_date, article_cn_text)\n",
    "\n",
    "\n",
    "        if num_article_publish_date < StartDate or webdata == \"[]\":\n",
    "            break      \n",
    "\n",
    "    print ('done all')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
